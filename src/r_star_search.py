import random
import logging
import json
from openai import OpenAI
from llama_cpp import Llama
import graphviz
import pandas as pd
# Initialize logging
logging.basicConfig(level=logging.INFO)

class RStarSearch:
    """
    A class implementing the R* Search algorithm, combining token generation with GPT-4 evaluation.
    """
    def __init__(self, user_query, generator_model_path, API_KEY, evaluator_model="gpt-4-1106-preview", max_tokens_per_turn=200, n_samples=5, parallel_limit=10, backtrack_threshold=10):
        """
        Initializes the RStarSearch algorithm with given parameters.

        :param user_query: The query to be answered.
        :param generator_model_path: File path to the generator model.
        :param API_KEY: OpenAI api key
        :param evaluator_model: Model name for the evaluator (GPT-4).
        :param max_tokens_per_turn: Maximum number of tokens to generate in each iteration.
        :param n_samples: Number of samples to generate per iteration.
        :param parallel_limit: Maximum number of parallel search paths.
        :param backtrack_threshold: Threshold for backtracking in the search algorithm.
        """
        self.user_query = user_query 
        self.max_tokens_per_turn = max_tokens_per_turn
        self.generator_model_path = generator_model_path
        self.evaluator_model = evaluator_model
        self.n_samples = n_samples
        self.parallel_limit = parallel_limit
        self.backtrack_threshold = backtrack_threshold
        
        self.generator_model = Llama(model_path=self.generator_model_path, n_ctx=4096, n_batch=512, n_threads=7, n_gpu_layers=2, verbose=False, seed=42)
        self.API_KEY = API_KEY
        self.evaluator_model_client = OpenAI(api_key=self.API_KEY)

        self.graph = graphviz.Digraph(comment='RStarSearch',format='pdf', graph_attr={'rankdir':'LR'})
        self.node_counter = 0 

    def add_to_graph(self, parent_node, current_node, label, path, color):
        """ Adds nodes and edges to the graph for visualisation """
        if parent_node:
            self.graph.edge(parent_node, current_node, label=f"{parent_node}{current_node}")
        self.graph.node(current_node, label=f"{label}\nPath: {path}", shape='box', color = color)
        # else:
            # self.graph.node(current_node, label=label)


    async def generate_tokens(self, continuation_until_now: str, max_tokens: int) -> list:
        """
        Generates n_choices of new tokens of random length and temperature tokens using the specified Open Source LM.

        :param continuation_until_now: The current state of the conversation or query.
        :param max_tokens: The maximum number of tokens to generate.
        :return: A list of generated token strings.
        """
        generated_texts = []
        prompt_template = """
        The following is a conversation with a helpful AI assistant that thinks step by step.
        Human: {}
        AI: {}
        """
        prompt = prompt_template.format(self.user_query, continuation_until_now)

        try:
            for _ in range(self.n_samples):
                token_max = random.randint(40, max_tokens)
                token_temp = random.random()
                response = self.generator_model(
                    prompt=prompt,
                    max_tokens=token_max,
                    temperature=token_temp,
                    stop=["Human:"],
                )
                choice_text = response["choices"][0]["text"].strip()
                logging.info(f"Generated token: {choice_text}, Max Tokens: {token_max}, Temperature: {token_temp}")
                generated_texts.append([choice_text,token_max, token_temp])
        except Exception as e:
            logging.error(f"Error during token generation: {e}")

        return generated_texts
    async def evaluate_with_gpt4(self, generated_choices):
        """
        Evaluates the generated choices using GPT-4. Read prompt for more information

        :param generated_choices: A list of strings representing the choices generated by the Llama model.
        :return: A list of evaluation scores or None if the evaluation fails.
        """
        formatted_choices = "\n".join([f"Choice {index}: {choice.strip()}" for index, choice in enumerate(generated_choices)])
        evaluation_prompt = {
                    "role": "system", 
                    "content": """
                    As an expert evaluator, you are presented with multiple response options generated by an AI assistant in response to a user's query. These responses are part of an iterative process aiming to develop a correct and complete solution. Your task is to evaluate each response's current state, considering its potential to evolve into a fully accurate and comprehensive answer.

                    For each response, provide:
                    1. A 'validity score' (v_score): Assess the intrinsic quality of the response, considering factors like correctness, creativity, accuracy, relevance, and coherence. Rate this on a scale of 0 to 10, where 0 represents an exemplary response, and 10 indicates a response with significant issues (off-topic, incoherent, or erroneous).

                    2. A 'heuristic score' (h_score): Evaluate how close the response is to a successful and complete solution. This score is not about the quality of the writing but rather about how much further development is needed. Score this on a scale of 0 to 10, with 0 indicating a response that successfully concludes the solution, and 10 suggesting the response is far from completion.

                    3. A 'wrong solution flag' (wrong_solution_flag): Indicate with a binary 'True' or 'False' whether the response, in its current form, is likely to lead the process toward an incorrect or inaccurate solution. 'True' should be used if the response has fundamental flaws that could misdirect the solution pathway, while 'False' indicates that the response, despite being incomplete, is on a potentially correct trajectory.

                    Please structure your evaluation as a JSON object, including brief justifications for each score and flag assigned. Your discerning analysis is crucial for guiding the iterative development of a correct and complete solution. Example:

                    {
                        "reason": "Here, briefly justify the assigned scores and flag, focusing on the response's potential to reach a complete and accurate solution.",
                        "scores": [
                                    {"choice": 0, "v_score": v, "h_score": h, "wrong_solution_flag": flag},
                                    ...
                                    {"choice": n, "v_score": v, "h_score": h, "wrong_solution_flag": flag}
                                ]
                    }
    
                    Note: It's important to remember that a response with a low v_score (high quality) can still be far from completion (high h_score), and a response that is nearing completion (low h_score) might still have significant issues (high v_score). The wrong_solution_flag should be reserved for responses that, if pursued, are likely to derail the process towards an inaccurate outcome.
                    """
                }

        user_prompt = {
            "role": "user",
            "content": f"The User asked: {self.user_query} Here are the responses to evaluate: {formatted_choices}. Your evaluations should be a JSON object"
        }

        for attempt in range(8):
            try:
                response = self.evaluator_model_client.chat.completions.create(model=self.evaluator_model, response_format={"type": "json_object"}, messages=[evaluation_prompt, user_prompt])
                evaluation_result = json.loads(response.choices[0].message.content)
                
                if "scores" in evaluation_result and all("v_score" in score and "h_score" in score for score in evaluation_result["scores"]):
                    logging.info(f"Evaluation result: {evaluation_result}")
                    return evaluation_result['scores']
                else:
                    logging.warning(f"Attempt {attempt + 1}: Invalid format in response.")
            except json.JSONDecodeError as e:
                logging.error(f"JSON decoding error in attempt {attempt + 1}: {e}")
            except Exception as e:
                logging.error(f"Unexpected error in attempt {attempt + 1}: {e}")

        logging.error("Failed to get a valid response after maximum retries.")
        return None

    async def expand_node(self, continuation_until_now, g_score, parent_node_id, path):
        """
        Expands a node in the search tree by generating new tokens and evaluating them.

        :param continuation_until_now: The current state of the conversation or query.
        :param g_score: The current path cost.
        :return: A list of paths with their associated scores.
        """
        new_continuations = await self.generate_tokens(continuation_until_now, self.max_tokens_per_turn)
        full_prompts_with_continuations = [continuation_until_now + " " + continuation[0] for continuation in new_continuations]
        scores = await self.evaluate_with_gpt4(full_prompts_with_continuations)

        paths_and_scores = []
        for index, score in enumerate(scores):
            total_cost = g_score + score['v_score']
            f_score = total_cost + score['h_score']
            new_continuation = continuation_until_now + " " + new_continuations[index][0]
            tokens = new_continuations[index][1]
            temperature = new_continuations[index][2]
            # node_id = f"Node{len(self.graph.nodes())}"
            node_id = f"Node{self.node_counter}"
            logging.info(f"{node_id}")
            self.node_counter += 1 
            new_path = f"{path}{node_id}"
            label = f"V:{score['v_score']}, H:{score['h_score']}, Flag:{score['wrong_solution_flag']}\n{new_continuations[index]}"
            if score['h_score'] == 0:
                self.add_to_graph(parent_node_id, node_id, label, new_path, color='green')
            elif score['wrong_solution_flag'] == True:
                self.add_to_graph(parent_node_id, node_id, label, new_path, color='red')
            else:
                self.add_to_graph(parent_node_id, node_id, label, new_path, color='black')
            path_info = {
                "continuation": new_continuation,
                "f_score": f_score,
                "g_score": total_cost,
                "h_score": score['h_score'],
                "v_score": score['v_score'],
                "wrong_solution_flag": score['wrong_solution_flag'],
                "node_id": node_id,
                "temperature": temperature,
                "max_tokens": tokens,
            }
            if score['wrong_solution_flag']:
                logging.info(f"Skipping expansion of node {node_id} due to wrong solution flag.")
                continue
            logging.info(f"Expanded path: {new_continuation}, Scores: {score}, node path: {new_path}, g_score (total cost): {total_cost}, f_score: {f_score}, temperature: {temperature}, max_tokens: {tokens}")
            paths_and_scores.append(path_info)

        return paths_and_scores

    
    async def run_search(self):
        """
        Runs the main R* search algorithm.

        :return: The final solution or indication that no solution was found.
        """
        open_queue = []  # Priority queue for managing nodes
        path_history = []  # To keep track of the path and scores
        data_for_csv = [] 
        initial_node_id = "Start"
        path = f"{initial_node_id}"
        self.add_to_graph(None, initial_node_id, f"User: {self.user_query} Start of Search",path, color="black")
        # Start with an initial state
        initial_state = (0, "", 0, [], initial_node_id)  # f_score, continuation, g_score, path_history
        open_queue.append(initial_state)

        while open_queue:
            current_state = open_queue.pop(0)
            f_score, continuation, g_score, path_history, node_id = current_state
            print(path_history)
            path = f"{node_id}"
            expanded_paths = await self.expand_node(continuation, g_score, node_id,path=path)
            for path in expanded_paths:
                data_entry = {
                    'continuation': path['continuation'],
                    'Node Path': path['node_id'],
                    'V Score': path['v_score'],
                    'H Score': path['h_score'],
                    'G Score': path['g_score'],
                    'F Score': path['f_score'],
                    'Temperature': path["temperature"],
                    'Max Tokens': path["max_tokens"],
                    'Wrong Solution Flag': path["wrong_solution_flag"]
                }
                data_for_csv.append(data_entry)
                if path['h_score'] == 0:
                    logging.info("Goal reached!")
                    file_name = f'r_gsmkk_meta_math_success{self.user_query.replace(" ", "_")[:10]}'
                    self.graph.render(file_name, view=True)
                    df = pd.DataFrame(data_for_csv)
                    csv_file_name = f'r_gsmkk_meta_math_success{self.user_query.replace(" ", "_")[:10]}.csv'
                    df.to_csv(csv_file_name, index=False)
                    logging.info(f"Data saved to {csv_file_name}")
                    return path['continuation']

                new_state = (path['f_score'], path['continuation'], path['g_score'], path_history + [path], path["node_id"])
                open_queue.append(new_state)
            open_queue.sort(key=lambda x: x[0])
        file_name = f'r_searcg_meta_math_fail_{self.user_query.replace(" ", "_")[:10]}'
        self.graph.render(file_name, view=True)
        logging.info("No solution found")
        df = pd.DataFrame(data_for_csv)
        csv_file_name = f'r_searcg_meta_math_fail_{self.user_query.replace(" ", "_")[:10]}.csv'
        df.to_csv(csv_file_name, index=False)
        logging.info(f"Data saved to {csv_file_name}")
        return "No solution found"
